import numpy as np


def initialize_parameters(layer_dims):
    return_dict = {}
    for i, (dim, ndim) in enumerate(zip(layer_dims, layer_dims[1:])):
        return_dict[i] = (np.random.randn(dim, ndim), np.zeros(dim))

    return return_dict


'''input: an array of the dimensions of each layer in the network (layer 0 is the size of the
flattened input, layer L is the output sigmoid)
output: a dictionary containing the initialized W and b parameters of each layer
(W1…WL, b1…bL).
Hint: Use the randn and zeros functions of numpy to initialize W and b, respectively'''


def linear_forward(A, W, b):
    linear_cache = {
        'A': A,
        'W': W,
        'b': b
    }
    # transposed_W = np.transpose(W)
    Z = W * A + b
    return Z, linear_cache


'''Description: Implement the linear part of a layer's forward propagation.
input:
A – the activations of the previous layer
W – the weight matrix of the current layer (of shape [size of current layer, size of
previous layer])
B – the bias vector of the current layer (of shape [size of current layer, 1])
Output:
Z – the linear component of the activation function (i.e., the value before applying the
non-linear function)
linear_cache – a dictionary containing A, W, b (stored for making the backpropagation
easier to compute)'''


def sigmoid(Z):
    return 1 / (1 + np.exp(-Z))


'''Input:
Z – the linear component of the activation function
Output:
A – the activations of the layer
activation_cache – returns Z, which will be useful for the backpropagation'''


def relu(Z):
    return max(0, Z)


'''
Input:
Z – the linear component of the activation function
Output:
A – the activations of the layer
activation_cache – returns Z, which will be useful for the backpropagation'''


def linear_activation_forward(A_prev, W, B, activation):
    Z, linear_cache = linear_forward(A_prev, W, B)
    activation_func = {
        'sigmoid': sigmoid,
        'relu': relu
    }
    A = activation_func[activation](Z)
    cache = {}
    cache['linear_cache'] = linear_cache
    cache['activation_cache'] = Z

    return A, cache


'''
Description:
Implement the forward propagation for the LINEAR->ACTIVATION layer
Input:
A_prev – activations of the previous layer
W – the weights matrix of the current layer
B – the bias vector of the current layer
Activation – the activation function to be used (a string, either “sigmoid” or “relu”)
Output:
A – the activations of the current layer
cache – a joint dictionary containing both linear_cache and activation_cache'''


def L_model_forward(X, parameters, use_batchnorm):
    caches = []
    list_of_AL = []
    num_of_examples = X.shape[1]
    for i in range(num_of_examples - 1):
        exapmle = X[:, i]
        A = exapmle
        for layer, params in parameters.items():
            if layer == len(parameters) - 1:
                break
            W, b = params
            A, cache = linear_activation_forward(A, W, b, 'relu')
            A = apply_batchnorm(A) if use_batchnorm else A
            caches.append(cache)
        W, b = parameters[len(parameters) - 1]
        AL, cache = linear_activation_forward(A, W, b, 'sigmoid')
        AL = apply_batchnorm(AL) if use_batchnorm else AL
        list_of_AL.append(AL)
        caches.append(cache)
    # TODO: AL maybe a matrix
    return list_of_AL, caches


'''Description:
Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID
computation
Input:
X – the data, numpy array of shape (input size, number of examples)
parameters – the initialized W and b parameters of each layer
use_batchnorm - a boolean flag used to determine whether to apply batchnorm after
the activation (note that this option needs to be set to “false” in Section 3 and “true” in
Section 4).
Output:
AL – the last post-activation value
caches – a list of all the cache objects generated by the linear_forward function'''


def compute_cost(AL, Y):
    m = len(AL)
    loss = 0
    for i in range(len(AL)):
        y = Y[i]
        al = AL[i]
        loss += y * np.log(al) + (1 - y) * np.log(1 - al)
    return loss * (-1 / m)


'''Description:
Implement the cost function defined by equation. (see the slides of the first lecture for
additional information if needed).
Input:
AL – probability vector corresponding to your label predictions, shape (1, number of
examples)
Y – the labels vector (i.e. the ground truth)
Output:
cost – the cross-entropy cost'''


def apply_batchnorm(A):
    return A / np.linalg.norm(A)


'''Description:
performs batchnorm on the received activation values of a given layer.
Input:
A - the activation values of a given layer
output:
NA - the normalized activation values, based on the formula learned in class'''


def Linear_backward(dZ, cache):
    A_prev = cache['A']
    W = cache['W']
    b = cache['b']
    m = len(b)
    dW = 1 / m * dZ * np.transpose(A_prev)
    db = 1 / m * np.sum(dZ)
    dA_prev = np.transpose(W) * dZ

    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation):
    if activation == 'sigmoid':
        dZ = sigmoid_backward(dA, cache['activation_cache'])
        dA_prev, dW, db = Linear_backward(dZ, cache['linear_cache'])
    else:
        dZ = relu_backward(dA, cache['activation_cache'])
        dA_prev, dW, db = Linear_backward(dZ, cache['linear_cache'])

    return dA_prev, dW, db


def relu_backward(dA, activation_cache):
    dZ = np.zeros(len(dA))
    Z = activation_cache
    dZ[Z > 0] = 1
    dZ = dZ * dA
    dZ[Z <= 0] = 0
    return dZ


def sigmoid_backward(dA, activation_cache):
    return sigmoid(dA) * (1 - sigmoid(dA))


def L_model_backward(AL, Y, caches):
    grads = {}
    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    cache = caches[-1]
    num_of_layers = len(caches) - 1
    dA_prev, dW, db = linear_activation_backward(dAL, cache, 'sigmoid')
    grads['dA' + str(num_of_layers)] = dA_prev
    grads['dW' + str(num_of_layers)] = dW
    grads['db' + str(num_of_layers)] = db
    for i, cache in enumerate(reversed(caches[:-1])):
        i = num_of_layers - i
        dA_prev, dW, db = linear_activation_backward(dAL, cache, 'relu')
        grads['dA' + str(i)] = dA_prev
        grads['dW' + str(i)] = dW
        grads['db' + str(i)] = db
    return grads


def update_parametrs(parameters, grad, learning_rate):
    return_params = {}
    for layer, params in parameters.items():
        currentDW = grad['dW' + str(layer)]
        currentDb = grad['db' + str(layer)]
        W, b = params
        W = W - learning_rate * currentDW
        b = b - learning_rate * currentDb
        return_params[layer] = (W, b)
    return return_params


def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size):
    costs = []

    for iter in num_iterations:
        X = [X[i:i + batch_size] for i in range(0, len(X), batch_size)]
        for x in X:
            parmeters = initialize_parameters(layers_dims)
            AL,caches = L_model_forward(x, parmeters, False)
            cost = compute_cost(AL, Y)
            grads = L_model_backward(AL, Y, caches)
            update_parametrs(parmeters, grads, learning_rate)
        if iter % 100 == 0:
            costs.append(cost)
    return parmeters, costs

def Predict(X, Y, parameters):




