import sys

import numpy as np
import pandas as pd
from keras.datasets import mnist

EPSILON = 0.01
def initialize_parameters(layer_dims):
    return_dict = {}
    for i, (dim, ndim) in enumerate(zip(layer_dims, layer_dims[1:])):
        return_dict[i] = (np.random.randn(ndim, dim), np.zeros((ndim,1)))

    return return_dict


'''input: an array of the dimensions of each layer in the network (layer 0 is the size of the
flattened input, layer L is the output sigmoid)
output: a dictionary containing the initialized W and b parameters of each layer
(W1…WL, b1…bL).
Hint: Use the randn and zeros functions of numpy to initialize W and b, respectively'''


def linear_forward(A, W, b):
    linear_cache = {
        'A': A,
        'W': W,
        'b': b
    }
    #transposed_W = np.transpose(W)
    Z = np.dot(W,A) + b
    return Z, linear_cache


'''Description: Implement the linear part of a layer's forward propagation.
input:
A – the activations of the previous layer
W – the weight matrix of the current layer (of shape [size of current layer, size of
previous layer])
B – the bias vector of the current layer (of shape [size of current layer, 1])
Output:
Z – the linear component of the activation function (i.e., the value before applying the
non-linear function)
linear_cache – a dictionary containing A, W, b (stored for making the backpropagation
easier to compute)'''


def softmax(x):
    """Compute softmax values for each sets of scores in x."""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()


def sigmoid(Z):
    return 1 / (1 + np.exp(-Z))


'''Input:
Z – the linear component of the activation function
Output:
A – the activations of the layer
activation_cache – returns Z, which will be useful for the backpropagation'''


def relu(Z):
    return np.maximum(0, Z)


'''
Input:
Z – the linear component of the activation function
Output:
A – the activations of the layer
activation_cache – returns Z, which will be useful for the backpropagation'''


def linear_activation_forward(A_prev, W, B, activation):
    Z, linear_cache = linear_forward(A_prev, W, B)
    activation_func = {
        'sigmoid': sigmoid,
        'relu': relu
    }
    A = activation_func[activation](Z)
    cache = {}
    cache['linear_cache'] = linear_cache
    cache['activation_cache'] = Z

    return A, cache


'''
Description:
Implement the forward propagation for the LINEAR->ACTIVATION layer
Input:
A_prev – activations of the previous layer
W – the weights matrix of the current layer
B – the bias vector of the current layer
Activation – the activation function to be used (a string, either “sigmoid” or “relu”)
Output:
A – the activations of the current layer
cache – a joint dictionary containing both linear_cache and activation_cache'''


def L_model_forward(X, parameters, use_batchnorm):
    caches = []
    L = len(parameters)
    A = X
    for i in range(L-1):
        A_prev = A
        W, b = parameters[i]
        A, cache = linear_activation_forward(A_prev, W, b, 'relu')
        A = apply_batchnorm(A) if use_batchnorm else A
        caches.append(cache)
    W, b = parameters[3]
    AL, cache = linear_activation_forward(A, W, b, 'sigmoid')
    AL = apply_batchnorm(AL) if use_batchnorm else AL
    caches.append(cache)
    # caches.append({
    #     'linear_cache':{
    #         'A':AL
    #     },
    #     'activation_cache':''
    # })
    # AL = softmax(AL)

    # TODO: AL maybe a matrix
    return AL, caches


'''Description:
Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID
computation
Input:
X – the data, numpy array of shape (input size, number of examples)
parameters – the initialized W and b parameters of each layer
use_batchnorm - a boolean flag used to determine whether to apply batchnorm after
the activation (note that this option needs to be set to “false” in Section 3 and “true” in
Section 4).
Output:
AL – the last post-activation value
caches – a list of all the cache objects generated by the linear_forward function'''


def compute_cost(AL, Y):
    m = len(AL)
    loss = 0
    for i in range(len(AL)):
        y = Y[i]
        al = AL[:,i]
        loss += np.dot(y , np.log(al)) + np.dot((1 - y) , np.log(1 - al))
    return loss * (-1 / m)


'''Description:
Implement the cost function defined by equation. (see the slides of the first lecture for
additional information if needed).
Input:
AL – probability vector corresponding to your label predictions, shape (1, number of
examples)
Y – the labels vector (i.e. the ground truth)
Output:
cost – the cross-entropy cost'''


def apply_batchnorm(A):
    return A / np.linalg.norm(A)


'''Description:
performs batchnorm on the received activation values of a given layer.
Input:
A - the activation values of a given layer
output:
NA - the normalized activation values, based on the formula learned in class'''


def Linear_backward(dZ, cache):
    A_prev = cache['A']
    W = cache['W']
    b = cache['b']
    m = len(b)
    dW = 1 / m * np.dot(dZ, np.transpose(A_prev))
    db = 1 / m * np.sum(dZ)
    dA_prev = np.dot(np.transpose(W), dZ)

    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation):
    if activation == 'sigmoid':
        dZ = sigmoid_backward(dA, cache['activation_cache'])
        dA_prev, dW, db = Linear_backward(dZ, cache['linear_cache'])
    else:
        dZ = relu_backward(dA, cache['activation_cache'])
        dA_prev, dW, db = Linear_backward(dZ, cache['linear_cache'])

    return dA_prev, dW, db


def relu_backward(dA, activation_cache):
    dZ = np.zeros(len(dA))
    Z = activation_cache
    dZ[Z > 0] = 1
    dZ = dZ * dA
    dZ[Z <= 0] = 0
    return dZ


def sigmoid_backward(dA, activation_cache):
    # np.diag(),10
    return sigmoid(dA) * (1 - sigmoid(dA))


def L_model_backward(AL, Y, caches):
    grads = {}
    Y = Y.reshape(AL.shape)
    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    cache = caches[-1]
    num_of_layers = len(caches) - 1
    dA_prev, dW, db = linear_activation_backward(dAL, cache, 'sigmoid')
    grads['dA' + str(num_of_layers)] = dA_prev
    grads['dW' + str(num_of_layers)] = dW
    grads['db' + str(num_of_layers)] = db
    caches = reversed(caches[:-1])
    for i, cache in enumerate(caches):
        i = num_of_layers - i - 1
        dA_prev, dW, db = linear_activation_backward(dAL, cache, 'relu')
        grads['dA' + str(i)] = dA_prev
        grads['dW' + str(i)] = dW
        grads['db' + str(i)] = db
    return grads


def update_parametrs(parameters, grad, learning_rate):
    return_params = {}
    for layer, params in parameters.items():
        currentDW = grad['dW' + str(layer)]
        currentDb = grad['db' + str(layer)]
        W, b = params
        W = W - learning_rate * currentDW
        b = b - learning_rate * currentDb
        return_params[layer] = (W, b)
    return return_params


def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size):
    costs = []
    parmeters = initialize_parameters(layers_dims)
    for iter in range(num_iterations):
        X = [X[:, i:i + batch_size] for i in range(0, X.shape[1], batch_size)]
        Y = [Y[i:i + batch_size] for i in range(0, len(Y), batch_size)]

        for x, y in zip(X, Y):
            AL, caches = L_model_forward(x, parmeters, False)
            cost = compute_cost(AL, y)
            grads = L_model_backward(AL, y, caches)
            parmeters = update_parametrs(parmeters, grads, learning_rate)
        if iter % 100 == 0 and iter > 0:
            prev_cost = costs[-1]
            costs.append(cost)
            print (cost)
            if prev_cost - cost <= EPSILON:
                break
    return parmeters, costs


def Predict(X, Y, parameters):
    AL, caches = L_model_forward(X, parameters, False)
    counter = 0
    for al, y in AL,Y:
        soft_al = softmax(al)
        prediction = np.argmax(soft_al)
        if prediction == y:
            counter += 1
    accuracy = counter / len(Y)
    return accuracy

def _get_date():
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    return x_train.reshape(-1, x_train.shape[0]),\
           y_train.reshape(-1, y_train.shape[0]),\
           x_test.reshape(-1, x_test.shape[0]),\
           y_test.reshape(-1, y_test.shape[0])

def pre_preprocess(y):
    y = y.tolist()
    flattened = [val for sublist in y for val in sublist]
    return pd.get_dummies(flattened).values

if __name__ == "__main__":
    layer_dims = [784, 20, 7, 5, 10]
    learning_rate = 0.009
    batch_size = 30
    iterations = 2000
    x_train, y_train, x_test, y_test = _get_date()
    y_train = pre_preprocess(y_train)
    y_test_hot = pre_preprocess(y_test)
    params, costs = L_layer_model(x_train, y_train, layer_dims, learning_rate, iterations, batch_size)
    accuracy = Predict(x_test, y_test, params)
    
    
    
    
    
