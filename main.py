import numpy as np



def initialize_parameters(layer_dims):
    return_dict = {}
    for i, (dim, ndim) in enumerate(zip(layer_dims,layer_dims[1:])):
            return_dict[i] = (np.random.randn(dim,ndim), np.zeros(dim))

    return return_dict
'''input: an array of the dimensions of each layer in the network (layer 0 is the size of the
flattened input, layer L is the output sigmoid)
output: a dictionary containing the initialized W and b parameters of each layer
(W1…WL, b1…bL).
Hint: Use the randn and zeros functions of numpy to initialize W and b, respectively'''



def linear_forward(A, W, b):
    linear_cache = {
        'A':A,
        'W':W,
        'b':b
    }
    #transposed_W = np.transpose(W)
    Z = W*A + b
    return Z, linear_cache
'''Description: Implement the linear part of a layer's forward propagation.
input:
A – the activations of the previous layer
W – the weight matrix of the current layer (of shape [size of current layer, size of
previous layer])
B – the bias vector of the current layer (of shape [size of current layer, 1])
Output:
Z – the linear component of the activation function (i.e., the value before applying the
non-linear function)
linear_cache – a dictionary containing A, W, b (stored for making the backpropagation
easier to compute)'''
def sigmoid(Z):
    return 1 / (1 + np.exp(-Z))
'''Input:
Z – the linear component of the activation function
Output:
A – the activations of the layer
activation_cache – returns Z, which will be useful for the backpropagation'''
def relu(Z):
    return max(0, Z)
'''
Input:
Z – the linear component of the activation function
Output:
A – the activations of the layer
activation_cache – returns Z, which will be useful for the backpropagation'''
def linear_activation_forward(A_prev, W, B, activation):
    Z, linear_cache = linear_forward(A_prev,W,B)
    activation_func = {
        'sigmoid':sigmoid,
        'relu': relu
    }
    A = activation_func[activation](Z)
    cache = {}
    cache['linear_cache'] = linear_cache
    cache['activation_cache'] = {
        'A': A,
        'W': W,
        'b': B
    }

    return A, cache
'''
Description:
Implement the forward propagation for the LINEAR->ACTIVATION layer
Input:
A_prev – activations of the previous layer
W – the weights matrix of the current layer
B – the bias vector of the current layer
Activation – the activation function to be used (a string, either “sigmoid” or “relu”)
Output:
A – the activations of the current layer
cache – a joint dictionary containing both linear_cache and activation_cache'''
def L_model_forward(X, parameters, use_batchnorm):
    caches = []
    list_of_AL = []
    num_of_examples = X.shape[1]
    for i in range(num_of_examples-1):
        exapmle = X[:,i]
        A = exapmle
        for layer,params in parameters.items():
            if layer == len(parameters) - 1:
                break
            W, b = params
            A, cache = linear_activation_forward(A, W, b, 'relu')
            A = apply_batchnorm(A) if use_batchnorm else A
            caches.append(cache)
        W, b = parameters[len(parameters) - 1]
        AL, cache = linear_activation_forward(A, W, b, 'sigmoid')
        AL = apply_batchnorm(AL) if use_batchnorm else AL
        list_of_AL.append(AL)
        caches.append(cache)
    #TODO: AL maybe a matrix
    return list_of_AL, caches

'''Description:
Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID
computation
Input:
X – the data, numpy array of shape (input size, number of examples)
parameters – the initialized W and b parameters of each layer
use_batchnorm - a boolean flag used to determine whether to apply batchnorm after
the activation (note that this option needs to be set to “false” in Section 3 and “true” in
Section 4).
Output:
AL – the last post-activation value
caches – a list of all the cache objects generated by the linear_forward function'''

def compute_cost(AL, Y):
    m = len(AL)
    loss = 0
    for i in range(len(AL)):
        y = Y[i]
        al = AL[i]
        loss += y*np.log(al) + (1-y)*np.log(1-al)
    return loss*(-1/m)
'''Description:
Implement the cost function defined by equation. (see the slides of the first lecture for
additional information if needed).
Input:
AL – probability vector corresponding to your label predictions, shape (1, number of
examples)
Y – the labels vector (i.e. the ground truth)
Output:
cost – the cross-entropy cost'''
def apply_batchnorm(A):
    return A/np.linalg.norm(A)
'''Description:
performs batchnorm on the received activation values of a given layer.
Input:
A - the activation values of a given layer
output:
NA - the normalized activation values, based on the formula learned in class'''